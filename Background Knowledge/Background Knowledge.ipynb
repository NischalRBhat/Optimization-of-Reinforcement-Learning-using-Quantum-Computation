{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Knowledge and Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Reinforcement Learning (RL), a dynamic subset of machine learning, is to teach agents how to make decisions sequentially in a setting to maximize cumulative rewards\n",
    " \\cite{p16}. This learning paradigm draws inspiration from human learning processes, where agents learn by interacting with their surroundings. Unlike supervised learning, where models learn from labeled data, or unsupervised learning, which uncovers patterns, RL is about learning from a continuous stream of actions and their outcomes. The agent communicates  with an environment by taking actions, getting the rewards as feedback, and using this information to refine its decision-making strategy over time. The ultimate goal  for the agent is to find an optimal policy, a set of rules dictating actions and that results in the highest long-term reward. Figure \\ref{fig:1} shows the interactions between agent and environment in an RL problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RL problem can be explained based on the reward hypothesis, which states that all goals may be characterized as the maximization of the expected cumulative reward, the goal of every RL agent is to maximize its expected cumulative reward (also known as expected return). To solve an RL problem, we have to find optimal policy \\cite{p30}.  The policy depicts the action for a chosen state. The optimal policy is the one which gives you the actions that maximize the expected return.\n",
    "There are two ways to find Optimal policy, i.e.  Value Based and Policy Based.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy-based methods: Directly train the policy to choose an action from a set of possible ones given a state (or probability distribution over possible actions at that state), as seen in Equation \\ref{eq:1}. No value function exists in this situation.\n",
    "\n",
    "$$\n",
    "\\text{State} \\Rightarrow \\text{Policy(State)} \\rightarrow \\text{Action}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Value-Based methods :The value of a state is the anticipated discounted return the agent can expect to receive if it starts at that state and then behaves in accordance with our rules \\cite{g1}. The following equation \\ref{eq:2} describes the value based function.\n",
    "\n",
    "$$\n",
    "   v_\\pi(s) = \\mathbf{E}_\\pi\\left[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\, \\middle| \\, S_t=s\\right]\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of value based methods. First one is the state-value function.The state-value function generates the anticipated result for each state if the agent starts at that state and then adheres to the policy continuously after that, described by the below equation \\ref{eq:3}. \n",
    "\n",
    "$$\n",
    "    V_\\pi(s) = \\mathbf{E}_\\pi\\left[G_t \\mid S_t=s\\right]\n",
    "$$\n",
    "\n",
    "The second one is Action Value Function.The action-value function, in the context of reinforcement learning, provides the expected return for a given state and action pair. It represents the expected cumulative reward that an agent would receive if it were to start in that state, take the specified action, and subsequently follow the policy indefinitely, as shown in Equation \\ref{eq:4}.\n",
    "\n",
    "$$\n",
    "    Q_\\pi(s, a) = \\mathbf{E}_\\pi\\left[G_t \\mid S_t=s, A_t=a\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical foundation of Reinforcement Learning (RL) is rooted in Markov Decision Processes (MDPs) \\cite{p24,p30} and the Bellman equation. MDPs provide a structured framework to model decision-making in uncertain environments. They encompass states, actions, transition probabilities, and rewards, encapsulating the dynamics of how an agent's actions influence its environment. MDP suggests that the action taken by our agent is independent of prior states and actions and only based on the current state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Bellman Equation to Calculate state-action values \\cite{p16}. It uses simplifies equation \\ref{eq:2} into a recursive form, that operates as follows: rather than computing the return for each state from the beginning, we can think of every state's value as:\n",
    "\n",
    "$R(t+1)$ is the initial reward, and $R(t+1) + (gamma * V(S(t+1))$ represents the discounted value of the subsequent state. Equation \\ref{eq:5} goes as following\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\mathbf{E}_{[\\pi]}\\left[R_{t+1} + \\gamma \\cdot V_\\pi(S_{t+1}) \\mid S_t=s\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is an off-policy value-based method that trains its value function using a value-based approach. Using an action-value function, the Q-learning algorithm identifies the value of a state and performs the corresponding action at that state \\cite{p16}. Algorithm \\ref{algo:1} explains the workflow for the Q-Learning.\n",
    "Q function uses Q-table for to get action for a corresponding state \\cite{w3}. Q table has the value for each state-action pair. The optimal value function is defined by equation \\ref{eq:6} below.\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg \\max_a Q^*(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Q-Learning Algorithm (Pseudo-code)\n",
    "def Q_Learning(pi, num_episodes, alpha, epsilon_list):\n",
    "    # Output: value function Q (≈ q_pi if num_episodes is large enough)\n",
    "    Q = initialize_Q()  # Initialize Q arbitrarily\n",
    "    for i in range(1, num_episodes + 1):\n",
    "        epsilon = epsilon_list[i]\n",
    "        S_t = observe_initial_state()\n",
    "        t = 0\n",
    "        while not is_terminal(S_t):\n",
    "            A_t = choose_action(Q, S_t, epsilon)  # e.g., ε-greedy\n",
    "            R_t1, S_t1 = take_action(A_t)\n",
    "            Q[S_t, A_t] = Q[S_t, A_t] + alpha * (R_t1 + gamma * max(Q[S_t1, a]) - Q[S_t, A_t])\n",
    "            S_t = S_t1\n",
    "            t += 1\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the domain of reinforcement learning, several notable extensions and adaptations of the core concepts have emerged. One significant method is the actor-critic approach, which cleverly integrates ideas from both policy gradient \\cite{p30} and value functions. Another refinement is the double Q-learning technique, which mitigates bias inherent in standard Q-learning by introducing an additional target action-value function.\n",
    "\n",
    "To enhance stability and sample efficiency, the introduction of an experience replay buffer is employed. This buffer allows for the storage and random sampling of experiences, contributing to improved learning quality.\n",
    "\n",
    "% Furthermore, this progression leads to the concept of offline or batch reinforcement learning, where the agent relies solely on a collection of previously acquired experiences instead of direct interaction with the environment. This approach is particularly practical when data generation is resource-intensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage Actor-Critic (A2C) is a reinforcement learning algorithm that combines elements of policy iteration and value iteration methods to efficiently train agents in sequential decision-making tasks. The key idea behind A2C is to have two components working in tandem: an actor and a critic. The actor is responsible for selecting actions, while the critic evaluates the chosen actions in terms of their advantage over alternative actions at each time step. The advantage function measures how much better a particular action is compared to the average expected return. By training the actor to maximize this advantage and simultaneously updating the critic to minimize the difference between the predicted and actual returns, A2C achieves a delicate balance between exploration and exploitation. \n",
    "\n",
    "Unlike traditional policy gradient methods, A2C utilizes a value function (critic) to reduce the variance in the gradient estimates, leading to more stable and faster convergence during training. Moreover, A2C can be parallelized efficiently, allowing for accelerated learning by updating the policy and value function parameters concurrently across multiple environments. This parallelization makes A2C a computationally efficient and scalable algorithm, well-suited for a wide range of reinforcement learning applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Computing definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the Quantum Computing from the basics. A classical computer (the one we are using) stores information in the form of 0s and 1s called bits, while a quantum computer encodes information in far richer states called qubits. While a single bit has 2 possible states, a qubit can be in any state on the surface of the sphere below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qubits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qubits are the core component of quantum computing. With superposition, we can encode an exponential amount of information that can scale a solution better than classical computing. These are the equivalent of bits in the classical computer but far more powerful.\n",
    "\n",
    "The integrity of quantum information is easily disrupted by environmental factors, as various elements interact. Despite the compact size of the quantum processor containing the qubits, substantial equipment is necessary to effectively shield and insulate these qubits from their surroundings. A single qubit can be represented using a three-dimensional representation called the Bloch Sphere, as seen in Figure \\ref{fig:2}.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The idea of superposition is crucial because qubits (quantum bits) rely on it.At the quantum level, when we measure the spin of a particle, it appears to be definitively either \"up\" or \"down.\" However, the quantum perspective reveals that the particle's spin actually exists in a superposition of states, encompassing both \"up\" and \"down\" simultaneously. This complex behavior is mathematically described, showing the probabilities of getting either outcome upon measurement. This uncertainty persists until the act of measurement collapses the superposition into one of the two possibilities. Despite its perplexing nature, this inherent unpredictability is an essential characteristic of the quantum realm, challenging our intuitive understanding of reality.\n",
    "\n",
    "So, when we say a particle's spin is in a superposition of states, what we really mean is that it is in a linear combination of up spin and down spin. The equation \\ref{eq:7} is shown in Dirac notation \\cite{p29}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha_0|0\\rangle + \\alpha_1|1\\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term amplitude refers to the coefficient. It has to deal with the possibility of being up spin or down spin as measured. In this case, the up and down spin states are simply the basis vectors (being 0 for up spin and 1 for sown spin), and Equation \\ref{eq:7} can be represented as given below. \n",
    "\n",
    "$$\n",
    "\\mid\\Psi\\rangle=\\alpha_0|0\\rangle+\\alpha_1|1\\rangle=\\left[\\begin{array}{l}\\alpha_0 \\\\ \\alpha_1\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orthogonal basis vectors $|0\\rangle$ and $|1\\rangle$ are represented as given below:\n",
    "\n",
    "$$\n",
    "|0\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right], \\quad|1\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Based on equation \\ref{eq:9}, we can get the following \n",
    "\n",
    "$$\n",
    "\\langle 0 \\mid 1\\rangle=0, \\quad\\langle 1 \\mid 1\\rangle=\\langle\\Psi \\mid \\Psi\\rangle=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using equations \\ref{eq:8} and \\ref{eq:9}, we can obtain more superposition states as shown below, along with the appropriate matrices.\n",
    "\n",
    "$$\n",
    "\\mid+\\rangle=\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "|-\\rangle=\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the superpositions between measurements. However, the superposition disintegrates into one of the potential states, i.e., either $|0\\rangle$ or $|1\\rangle$, when we measure the up spin. This is the core principle of quantum dynamics.\n",
    "\n",
    "We don't use conventional arithmetic or logical operations on qubits, in contrast to traditional computing. In quantum computing, there is no concept of a \"while statement\" or \"branching statement. Instead, we create unitary operators that employ the interference principle of quantum physics to control qubits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantum computers use a separate class of fundamental operators known as quantum gates. It ultimately comes down to manipulating, entangling, and measuring qubits in quantum computing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Let's look at the Hadamard gate, which is one of the most popular quantum gates. The linear operator is defined as the following matrix. The Hadamard gate itself is the transposed conjugate of the Hadamard. Below representation \\ref{eq:16} shows the matrix form of Hadamard gate\n",
    "\n",
    "$$\n",
    "H=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}\n",
    "1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entanglement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One counterintuitive idea can be seen in quantum mechanics. According to classical mechanics, it is possible to fully comprehend each subcomponent of a system before moving on to comprehend the entire system. Although in quantum mechanics we can't find the solution when we treated them as separate persons \\cite{g5}\n",
    "\n",
    "$$\n",
    "\\psi(x, y) \\neq \\psi(x) \\psi(y)\n",
    "$$\n",
    "\n",
    " We are aware of how components are correlated (entangled) when executing quantum operations. However, the precise quantities are unknown before any measurement is made. An efficient imitation of entangled correlations by a classical algorithm is expected to be much more difficult.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Quantum Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hybrid quantum-classical technique known as the variational quantum circuit makes use of the advantages of both quantum and classical computation \\cite{g6}. It is a particular class of quantum circuit with controllable parameters that a classical computer iteratively optimizes. These variables are visible.In artificial neural networks, they serve as the weights.\n",
    "\n",
    "The flexibility in circuit depth and some noise resistance of the variational quantum circuit technique have been demonstrated. It consists of encoding layers and entanglement layers (as in explained in equation \\ref{eq:17}). VQCs, often referred to as parameterized quantum circuits (PQC) in the literature, are a unique class of quantum circuits that have tunable parameters. The optimization techniques used to train such parameters were created in the traditional ML communities. Gradient-based or gradient-free optimization are both options. Quantum machine learning examples can make advantage of VQC.\n",
    "\\cite{q10} \\cite{q9}. Figure \\ref{fig:3} shows the general form of VQC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
