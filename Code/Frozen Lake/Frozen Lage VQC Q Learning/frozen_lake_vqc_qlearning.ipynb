{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from qiskit import QuantumCircuit, transpile, execute, Aer\n",
    "import qiskit\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "# Q-learning parameters\n",
    "num_episodes = 500\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Qubits and Qiskit Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum circuit parameters\n",
    "num_qubits = 4  # Adjust this based on your specific VQC design\n",
    "num_actions = 4  # Number of possible actions in the environment\n",
    "\n",
    "# Initialize Q-table\n",
    "num_states = env.observation_space.n\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "#print(q_table.size)\n",
    "\n",
    "# Quantum circuit setup\n",
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining VQC and Parameter encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan\n",
    "\n",
    "# Define the fixed variational part of the circuit with Rx and Cx gates\n",
    "def build_variational_circuit(params):\n",
    "    circuit = qiskit.circuit.library.TwoLocal(num_qubits, \"rx\", \"cz\", entanglement=\"circular\", reps=1, insert_barriers=True)\n",
    "    circuit = circuit.assign_parameters(params)\n",
    "    return circuit\n",
    "\n",
    "# Define a function to get fixed parameters based on the state\n",
    "def get_params(state):\n",
    "    # Example: Return an array of 8 fixed parameters based on the state\n",
    "    # You can customize this function to map states to specific parameter values\n",
    "    fixed_params = []\n",
    "    state_bin = format(state, '04b')\n",
    "    for i in range(4):\n",
    "        if state_bin[i] == '1':\n",
    "            fixed_params.append(atan(state))\n",
    "        else:\n",
    "            fixed_params.append(np.random.rand())\n",
    "    params = fixed_params + fixed_params\n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Objective Function for Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Objective function to maximize expected reward\n",
    "def objective_function():\n",
    "    # Initialize variables\n",
    "    rewards = []\n",
    "\n",
    "    # Q-learning loop\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        print(f\"Episode: {ep}\")\n",
    "        while not done:\n",
    "            # Encode the current state into the quantum state\n",
    "            state_binary = format(state, '04b')  # Assuming 4 qubits for state encoding\n",
    "            encoding_circuit = QuantumCircuit(num_qubits)\n",
    "            for i in range(num_qubits):\n",
    "                if state_binary[i] == '1':\n",
    "                    encoding_circuit.x(i)\n",
    "            \n",
    "            encoding_circuit.barrier()\n",
    "            encoding_circuit.h([0,1,2,3])\n",
    "            encoding_circuit.barrier()\n",
    "        \n",
    "            # Build the fixed variational circuit with fixed parameters based on the state\n",
    "            fixed_params = get_params(state)\n",
    "            variational_circuit = build_variational_circuit(fixed_params)\n",
    "\n",
    "            # Combine encoding and variational parts of the circuit\n",
    "            quantum_circuit = encoding_circuit.compose(variational_circuit)\n",
    "            quantum_circuit.measure_all()\n",
    "\n",
    "            # Simulate the circuit to obtain measurement outcomes\n",
    "            transpiled_circuit = transpile(quantum_circuit, backend)\n",
    "            result = execute(transpiled_circuit, backend, shots=32768).result()\n",
    "            counts = result.get_counts(quantum_circuit)\n",
    "\n",
    "            # Map measurement outcomes to actions (customize as needed)\n",
    "            action_counts = {'00': 0, '01': 0, '10': 0, '11': 0}\n",
    "            for outcome, count in counts.items():\n",
    "                first_two_bits = outcome[:2]\n",
    "                if first_two_bits in action_counts:\n",
    "                    action_counts[first_two_bits] += count\n",
    "\n",
    "            action = int(max(action_counts, key=lambda x: action_counts[x]), 2)\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()  # Explore\n",
    "\n",
    "            # Take the selected action and observe the next state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update the Q-table using Q-learning update rule\n",
    "            q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n",
    "                learning_rate * (reward + discount_factor * np.max(q_table[next_state, :]))\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                state = env.reset()  # Reset the environment when an episode is done\n",
    "                break\n",
    "            else:\n",
    "                state = next_state  # Update the current state if the episode is not done\n",
    "\n",
    "    env.close()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = objective_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Q table into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"q_table.pkl\"\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(q_table, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
